{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbbf2c3",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7806c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (1212, 832), y_train.shape (1212, 1), X_test.shape (776, 832)\n",
      "              x0            x1           x2             x3          x4  \\\n",
      "id                                                                       \n",
      "0   14168.823171  10514.380717  3316.149698   94230.695124  102.386606   \n",
      "1   17757.037554           NaN  4101.016273   92959.527633         NaN   \n",
      "2   14226.656663  11029.642499          NaN  124055.600561  100.542483   \n",
      "3    8766.012436   7384.202998  2147.308418  100157.719990  104.855061   \n",
      "4   13801.016418  13269.493652  3408.316953   92048.527786  103.759758   \n",
      "\n",
      "            x5            x6            x7            x8         x9  ...  \\\n",
      "id                                                                   ...   \n",
      "0    92.677127  11108.748199  10866.505510  10837.622093  10.227734  ...   \n",
      "1    99.855168  10013.959449  10826.607494  10076.101597  11.436970  ...   \n",
      "2    92.860892           NaN  10492.342868           NaN  10.810076  ...   \n",
      "3   101.929026  10050.049932  10499.521099  10525.030989  10.092109  ...   \n",
      "4    95.789235   9667.353978  10750.783106  10618.800750  12.006773  ...   \n",
      "\n",
      "            x822          x823        x824        x825        x826  \\\n",
      "id                                                                   \n",
      "0            NaN  12352.094085  846.014651  105.132144  102.112809   \n",
      "1            NaN  16198.071494  776.084467  106.385590  103.472030   \n",
      "2   10329.704431  13976.063780  737.040332  103.671234  109.458246   \n",
      "3   10008.251395   6212.127347  329.044233  105.084488  104.858546   \n",
      "4   10095.782015  13772.061493         NaN         NaN  100.369834   \n",
      "\n",
      "           x827      x828         x829         x830          x831  \n",
      "id                                                                 \n",
      "0   2090.004260  2.691845  1234.374109  1000.784475   9285.751272  \n",
      "1   2474.051881  2.287976          NaN  1012.626705  11750.284764  \n",
      "2   2656.083281  2.843706   888.353607  1048.810385   9553.922728  \n",
      "3   1097.785204  2.732257   927.752967  1048.357330           NaN  \n",
      "4   2693.053231  2.702908  1471.354073  1071.284484   9423.533063  \n",
      "\n",
      "[5 rows x 832 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "X_test = data_dir / \"X_test.csv\"\n",
    "y_train = data_dir / \"y_train.csv\"\n",
    "X_train = data_dir / \"X_train.csv\"\n",
    "\n",
    "X_test = pd.read_csv(X_test, index_col=\"id\")\n",
    "X_test.index = X_test.index.astype(int)\n",
    "\n",
    "y_train = pd.read_csv(y_train, index_col=\"id\")\n",
    "y_train.index = y_train.index.astype(int)\n",
    "\n",
    "X_train = pd.read_csv(X_train, index_col=\"id\")\n",
    "X_train.index = X_train.index.astype(int)\n",
    "\n",
    "print(f\"X_train.shape {X_train.shape}, y_train.shape {y_train.shape}, X_test.shape {X_test.shape}\")\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de7d5437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#have a look at the data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train.values, bins=50)\n",
    "plt.xlabel(\"Target Variable\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Target Variable\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b28cc",
   "metadata": {},
   "source": [
    "# 1. imputing missing values\n",
    "\n",
    "First we impute nan values with the mean for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef5a23ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan-values: 0\n",
      "              x0            x1           x2             x3          x4  \\\n",
      "id                                                                       \n",
      "0   14168.823171  10514.380717  3316.149698   94230.695124  102.386606   \n",
      "1   17757.037554  10950.160761  4101.016273   92959.527633  105.070358   \n",
      "2   14226.656663  11029.642499  3430.837498  124055.600561  100.542483   \n",
      "3    8766.012436   7384.202998  2147.308418  100157.719990  104.855061   \n",
      "4   13801.016418  13269.493652  3408.316953   92048.527786  103.759758   \n",
      "\n",
      "            x5            x6            x7            x8         x9  ...  \\\n",
      "id                                                                   ...   \n",
      "0    92.677127  11108.748199  10866.505510  10837.622093  10.227734  ...   \n",
      "1    99.855168  10013.959449  10826.607494  10076.101597  11.436970  ...   \n",
      "2    92.860892   9983.055476  10492.342868  10495.835570  10.810076  ...   \n",
      "3   101.929026  10050.049932  10499.521099  10525.030989  10.092109  ...   \n",
      "4    95.789235   9667.353978  10750.783106  10618.800750  12.006773  ...   \n",
      "\n",
      "            x822          x823        x824        x825        x826  \\\n",
      "id                                                                   \n",
      "0   10069.191241  12352.094085  846.014651  105.132144  102.112809   \n",
      "1   10069.191241  16198.071494  776.084467  106.385590  103.472030   \n",
      "2   10329.704431  13976.063780  737.040332  103.671234  109.458246   \n",
      "3   10008.251395   6212.127347  329.044233  105.084488  104.858546   \n",
      "4   10095.782015  13772.061493  812.316152  104.968652  100.369834   \n",
      "\n",
      "           x827      x828         x829         x830          x831  \n",
      "id                                                                 \n",
      "0   2090.004260  2.691845  1234.374109  1000.784475   9285.751272  \n",
      "1   2474.051881  2.287976  1359.981226  1012.626705  11750.284764  \n",
      "2   2656.083281  2.843706   888.353607  1048.810385   9553.922728  \n",
      "3   1097.785204  2.732257   927.752967  1048.357330   9981.085085  \n",
      "4   2693.053231  2.702908  1471.354073  1071.284484   9423.533063  \n",
      "\n",
      "[5 rows x 832 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train.fillna(pd.Series(X_train.mean()), inplace=True)\n",
    "print(f\"Nan-values: {X_train.isna().sum().sum()}\")\n",
    "print(X_train.head())\n",
    "X_test.fillna(pd.Series(X_test.mean()), inplace=True)\n",
    "y_train.fillna(pd.Series(y_train.mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d4ae2",
   "metadata": {},
   "source": [
    "# 2. Outlier detection\n",
    "Next we build an outlier detection model to classify samples as outliers and eventually remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c2064cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier indices: {np.int64(3), np.int64(515), np.int64(645), np.int64(900), np.int64(1031), np.int64(648), np.int64(906), np.int64(397), np.int64(917), np.int64(278), np.int64(24), np.int64(1048), np.int64(30), np.int64(416), np.int64(1056), np.int64(1058), np.int64(805), np.int64(167), np.int64(297), np.int64(681), np.int64(428), np.int64(306), np.int64(821), np.int64(694), np.int64(697), np.int64(953), np.int64(573), np.int64(190), np.int64(575), np.int64(64), np.int64(321), np.int64(448), np.int64(837), np.int64(70), np.int64(583), np.int64(75), np.int64(460), np.int64(972), np.int64(1102), np.int64(335), np.int64(474), np.int64(91), np.int64(731), np.int64(860), np.int64(991), np.int64(480), np.int64(225), np.int64(1120), np.int64(100), np.int64(229), np.int64(740), np.int64(231), np.int64(234), np.int64(362), np.int64(1137), np.int64(114), np.int64(882), np.int64(502), np.int64(1144), np.int64(506), np.int64(1150)}\n",
      "Number of outliers detected: 61\n"
     ]
    }
   ],
   "source": [
    "#outlier detection using isolation forest and guessing the contamination to be 5%\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "outlier_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "outlier_model.fit(X_train)\n",
    "outlier_preds = outlier_model.predict(X_train)\n",
    "isoforest_outlier_indices = set(np.where(outlier_preds == -1)[0])\n",
    "\n",
    "print(\"Outlier indices:\", isoforest_outlier_indices)\n",
    "\n",
    "print(f\"Number of outliers detected: {len(isoforest_outlier_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cac8194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Outlier indices: [  16   30   32   56   70   72   88   91  142  168  180  207  211  214\n",
      "  245  278  313  321  334  352  397  403  404  431  518  537  545  553\n",
      "  570  574  583  616  622  631  633  635  681  694  731  732  741  755\n",
      "  771  805  806  808  836  872  882  888  899  900  906  917  949  953\n",
      "  972  990  994 1001 1038 1066 1081 1087 1102 1132 1135 1144 1205]\n",
      "Number of outliers detected by SVM: 69\n",
      "Number of common outliers detected by both methods: 19\n",
      "Common outlier indices: {np.int64(900), np.int64(906), np.int64(397), np.int64(917), np.int64(278), np.int64(30), np.int64(805), np.int64(681), np.int64(694), np.int64(953), np.int64(321), np.int64(70), np.int64(583), np.int64(972), np.int64(1102), np.int64(731), np.int64(91), np.int64(882), np.int64(1144)}\n"
     ]
    }
   ],
   "source": [
    "# outlier detection using OneClassSVM\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "svm_model = OneClassSVM(nu=0.01, kernel=\"rbf\", gamma=\"scale\")\n",
    "svm_model.fit(X_train_scaled)\n",
    "svm_preds = svm_model.predict(X_train_scaled)\n",
    "\n",
    "print(\"SVM Outlier indices:\", np.where(svm_preds == -1)[0])\n",
    "\n",
    "svm_outlier_indices = set(np.where(svm_preds == -1)[0])\n",
    "\n",
    "print(f\"Number of outliers detected by SVM: {np.sum(svm_preds == -1)}\")\n",
    "\n",
    "# Comparing the two methods\n",
    "common_outliers = isoforest_outlier_indices & svm_outlier_indices\n",
    "print(f\"Number of common outliers detected by both methods: {len(common_outliers)}\")\n",
    "print(\"Common outlier indices:\", common_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71f5a894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned X_train shape: (1193, 832), Cleaned y_train shape: (1193, 1)\n"
     ]
    }
   ],
   "source": [
    "#removing outliers from the training data\n",
    "X_train_cleaned = X_train.drop(index=X_train.index[list(common_outliers)])\n",
    "y_train_cleaned = y_train.drop(index=y_train.index[list(common_outliers)])\n",
    "print(f\"Cleaned X_train shape: {X_train_cleaned.shape}, Cleaned y_train shape: {y_train_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4d015",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "this step is harder than imagined, a new idea came to my mind:\n",
    "\n",
    "1: select a subset of features randomly, train and score the predictions, as well as the features used\n",
    "\n",
    "2: do it for another subset\n",
    "\n",
    "3: mark the features which occur in higher scored subsets and use these for the final training (Monte Carlo aproach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eebc46c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features to drop: ['x69', 'x132', 'x151', 'x202', 'x230', 'x231', 'x240', 'x250', 'x278', 'x288', 'x316', 'x317', 'x333', 'x334', 'x348', 'x374', 'x392', 'x414', 'x442', 'x479', 'x502', 'x505', 'x562', 'x568', 'x579', 'x610', 'x614', 'x634', 'x668', 'x675', 'x698', 'x715', 'x727', 'x774', 'x780', 'x790']\n",
      "Number of features to drop: 36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def correlation_matrix_reduction(df, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features (one from each pair).\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input feature DataFrame\n",
    "    threshold (float): Correlation threshold\n",
    "    \n",
    "    Returns:\n",
    "    list: Features to drop\n",
    "    \"\"\"\n",
    "    # Compute absolute correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "    \n",
    "    # Create upper triangle mask\n",
    "    mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    upper = corr_matrix.where(mask)\n",
    "    \n",
    "    # Find columns with any correlation > threshold in upper triangle\n",
    "    to_drop = [col for col in upper.columns if (upper[col] > threshold).any()]\n",
    "    \n",
    "    print(\"Highly correlated features to drop:\", to_drop)\n",
    "    print(f\"Number of features to drop: {len(to_drop)}\")\n",
    "    \n",
    "    return to_drop\n",
    "\n",
    "\n",
    "to_drop_corr = correlation_matrix_reduction(X_train_cleaned, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adc2f948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low variance features to drop: ['x33', 'x67', 'x68', 'x104', 'x129', 'x147', 'x166', 'x179', 'x183', 'x185', 'x189', 'x195', 'x196', 'x207', 'x208', 'x229', 'x262', 'x271', 'x281', 'x289', 'x292', 'x295', 'x296', 'x305', 'x307', 'x336', 'x338', 'x363', 'x384', 'x385', 'x446', 'x450', 'x462', 'x466', 'x489', 'x492', 'x500', 'x519', 'x529', 'x530', 'x550', 'x556', 'x580', 'x583', 'x586', 'x593', 'x605', 'x620', 'x625', 'x628', 'x666', 'x679', 'x682', 'x709', 'x724', 'x755', 'x803', 'x808', 'x815']\n",
      "Number of low variance features to drop: 59\n",
      "Final features to drop: ['x790', 'x33', 'x385', 'x614', 'x278', 'x450', 'x625', 'x132', 'x556', 'x586', 'x229', 'x682', 'x568', 'x230', 'x715', 'x583', 'x384', 'x530', 'x593', 'x262', 'x363', 'x240', 'x307', 'x610', 'x334', 'x289', 'x579', 'x666', 'x189', 'x724', 'x502', 'x492', 'x183', 'x166', 'x202', 'x69', 'x317', 'x774', 'x780', 'x392', 'x207', 'x316', 'x414', 'x727', 'x295', 'x67', 'x755', 'x442', 'x292', 'x550', 'x628', 'x104', 'x462', 'x305', 'x195', 'x580', 'x208', 'x338', 'x271', 'x675', 'x196', 'x68', 'x620', 'x296', 'x129', 'x479', 'x336', 'x668', 'x466', 'x446', 'x179', 'x333', 'x815', 'x709', 'x374', 'x803', 'x808', 'x231', 'x562', 'x505', 'x519', 'x698', 'x348', 'x151', 'x185', 'x634', 'x529', 'x489', 'x288', 'x147', 'x250', 'x281', 'x500', 'x679', 'x605']\n"
     ]
    }
   ],
   "source": [
    "#remove low variance features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "def low_variance_reduction(threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    selector.fit(X_train_cleaned)\n",
    "    to_keep = X_train_cleaned.columns[selector.get_support()]\n",
    "    to_drop = [col for col in X_train_cleaned.columns if col not in to_keep]\n",
    "    print(\"Low variance features to drop:\", to_drop)\n",
    "    print(f\"Number of low variance features to drop: {len(to_drop)}\")\n",
    "    return to_drop\n",
    "to_drop_var = low_variance_reduction()\n",
    "\n",
    "final_to_drop = list(set(to_drop_corr) | set(to_drop_var))\n",
    "print(\"Final features to drop:\", final_to_drop)\n",
    "X_selected = X_train_cleaned.drop(columns=final_to_drop, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c62b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Train quick XGBoost model\n",
    "quick_model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "file = Path(\"results/feature_importances.csv\")\n",
    "\n",
    "if file.exists():\n",
    "    importances = pd.read_csv(file, index_col=0).squeeze()\n",
    "\n",
    "else:\n",
    "    quick_model.fit(X_selected, y_train_cleaned)\n",
    "\n",
    "    # Get permutation importance\n",
    "    result = permutation_importance(\n",
    "        quick_model, \n",
    "        X_selected, \n",
    "        y_train_cleaned, \n",
    "        n_repeats=5, \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    # Extract and rank features\n",
    "    importances = pd.Series(result.importances_mean, index=X_selected.columns)\n",
    "    #store  the importances\n",
    "    importances.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "356d8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_features(top_n=50):    \n",
    "    top_features = importances.nlargest(top_n).index.tolist()\n",
    "    # Select top features\n",
    "    return top_features\n",
    "\n",
    "n_features_xgb = 70\n",
    "features_xgb = select_top_features(n_features_xgb)\n",
    "n_features_nnet = 0\n",
    "features_nnet = select_top_features(n_features_nnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f1ed81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cédric\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features selected for Neural Network: Index(['x287', 'x349', 'x766', 'x146', 'x742', 'x306', 'x654', 'x242', 'x194',\n",
      "       'x410', 'x641', 'x702', 'x415', 'x465', 'x485', 'x507', 'x458', 'x133',\n",
      "       'x159', 'x115'],\n",
      "      dtype='object')\n",
      "Intersection of features selected by MI and Permutation Importance:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "mi_scores = mutual_info_regression(X_selected, y_train_cleaned)\n",
    "top_features = X_selected.columns[np.argsort(mi_scores)[-20:]]\n",
    "print(\"Top features selected for Neural Network:\", top_features)\n",
    "\n",
    "features_nnet = list(set(features_nnet) & set(top_features))\n",
    "print(\"Intersection of features selected by MI and Permutation Importance:\\n\", features_nnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f0ae5",
   "metadata": {},
   "source": [
    "# Main Task, fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03c45a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MULTI-MODEL COMPARISON FOR BRAIN MRI AGE PREDICTION ===\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import Ridge\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "# --- 1. Split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_selected, y_train_cleaned, test_size=0.15, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f0d3861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ XGB | Train R²: 0.9982 | Test R²: 0.5848 | Test MAE: 4.6534\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "#1. fit the XGB model with the selected features and hyperparameters found previously\n",
    "xgb_params ={\n",
    "    'subsample'       : 0.6,\n",
    "    'reg_alpha'       : 0.6,\n",
    "    'n_estimators'    : 600,\n",
    "    'max_depth'       : 9,\n",
    "    'learning_rate'   : 0.02,\n",
    "    'gamma'           : 1,\n",
    "    'colsample_bytree': 0.7,\n",
    "    }\n",
    "X_train_xgb = X_train[features_xgb]\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "xgb.fit(X_train_xgb, y_train)\n",
    "\n",
    "y_train_pred_xgb = xgb.predict(X_train_xgb)\n",
    "y_pred_xgb = xgb.predict(X_val[features_xgb])\n",
    "\n",
    "print(f\"→ XGB | Train R²: {r2_score(y_train, y_train_pred_xgb):.4f} | Test R²: {r2_score(y_val, y_pred_xgb):.4f} | Test MAE: {mean_absolute_error(y_val, y_pred_xgb):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ac3967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ kernelridge | Test R²: 0.3826 | Test MAE: 5.6766\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------------------------------\n",
    "#2. Train the aproximated kernel model\n",
    "pipe_nys = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nystroem\", Nystroem(kernel=\"rbf\")),\n",
    "    (\"ridge\", Ridge())  # Add regressor for prediction\n",
    "])\n",
    "\n",
    "nys_params = {'nystroem__gamma': np.float64(0.001), 'nystroem__n_components': 550, 'ridge__alpha': 0.1}\n",
    "pipe_nys.set_params(**nys_params)\n",
    "pipe_nys.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_nys = pipe_nys.predict(X_train)\n",
    "y_pred_nys = pipe_nys.predict(X_val)\n",
    "\n",
    "results = ({\n",
    "    \"Train R²\": r2_score(y_train, y_train_pred_nys),\n",
    "    \"Test R²\": r2_score(y_val, y_pred_nys),\n",
    "    \"Test MAE\": mean_absolute_error(y_val, y_pred_nys),\n",
    "    \"Params\": nys_params\n",
    "})\n",
    "\n",
    "print(f\"→ kernelridge | Test R²: {r2_score(y_val, y_pred_nys):.4f} | Test MAE: {mean_absolute_error(y_val, y_pred_nys):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "210cd4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cédric\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ RandomForest | Test R²: 0.4935 | Test MAE: 5.0942\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#3. Train RandomForest\n",
    "rndm_frst_params = {\n",
    "    'model__n_estimators'     : 250,\n",
    "    'model__min_samples_split': 6,\n",
    "    'model__min_samples_leaf' : 1,\n",
    "    'model__max_depth'        : None\n",
    "      }\n",
    "pipe_rndm_frst = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", RandomForestRegressor())\n",
    "])\n",
    "\n",
    "pipe_rndm_frst.set_params(**rndm_frst_params)\n",
    "pipe_rndm_frst.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_rndm_frst = pipe_rndm_frst.predict(X_train)\n",
    "y_pred_rndm_frst = pipe_rndm_frst.predict(X_val)\n",
    "\n",
    "results = ({\n",
    "    \"Train R²\": r2_score(y_train, y_train_pred_rndm_frst),\n",
    "    \"Test R²\": r2_score(y_val, y_pred_rndm_frst),\n",
    "    \"Test MAE\": mean_absolute_error(y_val, y_pred_rndm_frst),\n",
    "    \"Params\": rndm_frst_params\n",
    "})\n",
    "\n",
    "print(f\"→ RandomForest | Test R²: {r2_score(y_val, y_pred_rndm_frst):.4f} | Test MAE: {mean_absolute_error(y_val, y_pred_rndm_frst):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "775b3ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cédric\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ SVR | Test R²: 0.3819 | Test MAE: 5.6448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "#4. next we add a SVMR into the mix\n",
    "\n",
    "svr_params = {'model__kernel': 'rbf', 'model__gamma': 'scale', 'model__epsilon': 0.18, 'model__C': 20}\n",
    "pipe_svr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SVR())\n",
    "])\n",
    "\n",
    "pipe_svr.set_params(**svr_params)\n",
    "pipe_svr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_svr = pipe_svr.predict(X_train)\n",
    "y_pred_svr = pipe_svr.predict(X_val)\n",
    "\n",
    "results = ({\n",
    "    \"Train R²\": r2_score(y_train, y_train_pred_svr),\n",
    "    \"Test R²\": r2_score(y_val, y_pred_svr),\n",
    "    \"Test MAE\": mean_absolute_error(y_val, y_pred_svr),\n",
    "    \"Params\": svr_params\n",
    "})\n",
    "\n",
    "print(f\"→ SVR | Test R²: {r2_score(y_val, y_pred_svr):.4f} | Test MAE: {mean_absolute_error(y_val, y_pred_svr):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_model_preds_train = {\n",
    "    \"XGB_Pred\": y_train_pred_xgb,\n",
    "    \"NYS_Pred\": y_train_pred_nys,\n",
    "    \"RndFrst\": y_train_pred_rndm_frst,\n",
    "    \"SVR\": y_train_pred_svr,\n",
    "}\n",
    "\n",
    "primary_model_preds = {\n",
    "    \"XGB_Pred\": y_pred_xgb,\n",
    "    \"NYS_Pred\": y_pred_nys,\n",
    "    \"RndFrst\": y_pred_rndm_frst,\n",
    "    \"SVR\": y_pred_svr,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b3e728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple MLP module\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, activation, hidden_units=128, num_layers=2, dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_units), activation(), nn.Dropout(dropout)]\n",
    "        for _ in range(1, num_layers):\n",
    "            layers.extend([nn.Linear(hidden_units, hidden_units), activation(), nn.Dropout(dropout)])\n",
    "        layers.append(nn.Linear(hidden_units, 1))\n",
    "        layers.append(nn.Linear(1, 1))  # 1-1 linear layer for scaling/shifting\n",
    "        layers.append(nn.Softplus())    # Softplus activation to ensure positive output\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using Xavier for linear layers\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df72c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering 0 primary features for NN\n",
      "Input layer dim for NN: 4\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m4141.8848\u001b[0m     \u001b[32m3266.5587\u001b[0m  0.0973\n",
      "      2     \u001b[36m2490.8038\u001b[0m     \u001b[32m1525.1093\u001b[0m  0.1105\n",
      "      3      \u001b[36m912.1351\u001b[0m      \u001b[32m356.9157\u001b[0m  0.1005\n",
      "      4      \u001b[36m186.1297\u001b[0m       \u001b[32m88.9712\u001b[0m  0.1002\n",
      "      5       \u001b[36m53.3955\u001b[0m       \u001b[32m32.5963\u001b[0m  0.1010\n",
      "      6       \u001b[36m30.5995\u001b[0m       \u001b[32m28.2809\u001b[0m  0.1173\n",
      "      7       \u001b[36m27.7534\u001b[0m       \u001b[32m27.3135\u001b[0m  0.1041\n",
      "      8       \u001b[36m26.3829\u001b[0m       \u001b[32m26.4627\u001b[0m  0.1207\n",
      "      9       \u001b[36m24.8213\u001b[0m       \u001b[32m25.0657\u001b[0m  0.1070\n",
      "     10       \u001b[36m22.8100\u001b[0m       \u001b[32m23.5623\u001b[0m  0.1032\n",
      "     11       \u001b[36m21.3351\u001b[0m       \u001b[32m22.6337\u001b[0m  0.1071\n",
      "     12       \u001b[36m20.2300\u001b[0m       \u001b[32m21.7694\u001b[0m  0.1024\n",
      "     13       \u001b[36m19.1783\u001b[0m       \u001b[32m20.7100\u001b[0m  0.1035\n",
      "     14       \u001b[36m18.0139\u001b[0m       \u001b[32m19.3886\u001b[0m  0.1072\n",
      "     15       \u001b[36m16.6420\u001b[0m       \u001b[32m17.7975\u001b[0m  0.1042\n",
      "     16       \u001b[36m15.1570\u001b[0m       \u001b[32m16.2150\u001b[0m  0.1152\n",
      "     17       \u001b[36m13.7533\u001b[0m       \u001b[32m14.6556\u001b[0m  0.0991\n",
      "     18       \u001b[36m12.3326\u001b[0m       \u001b[32m13.0602\u001b[0m  0.0964\n",
      "     19       \u001b[36m10.8851\u001b[0m       \u001b[32m11.3090\u001b[0m  0.1771\n",
      "     20        \u001b[36m9.2615\u001b[0m        \u001b[32m9.4200\u001b[0m  0.0942\n",
      "     21        \u001b[36m7.6489\u001b[0m        \u001b[32m7.6604\u001b[0m  0.0973\n",
      "     22        \u001b[36m6.0784\u001b[0m        \u001b[32m6.0004\u001b[0m  0.0943\n",
      "     23        \u001b[36m4.6652\u001b[0m        \u001b[32m4.6491\u001b[0m  0.1033\n",
      "     24        \u001b[36m3.6089\u001b[0m        \u001b[32m3.7056\u001b[0m  0.1103\n",
      "     25        \u001b[36m2.8825\u001b[0m        \u001b[32m3.0429\u001b[0m  0.1154\n",
      "     26        \u001b[36m2.3620\u001b[0m        \u001b[32m2.5560\u001b[0m  0.1089\n",
      "     27        \u001b[36m1.9722\u001b[0m        \u001b[32m2.1828\u001b[0m  0.1100\n",
      "     28        \u001b[36m1.6709\u001b[0m        \u001b[32m1.8913\u001b[0m  0.1052\n",
      "     29        \u001b[36m1.4354\u001b[0m        \u001b[32m1.6609\u001b[0m  0.1222\n",
      "     30        \u001b[36m1.2494\u001b[0m        \u001b[32m1.4755\u001b[0m  0.0984\n",
      "     31        \u001b[36m1.0999\u001b[0m        \u001b[32m1.3241\u001b[0m  0.1003\n",
      "     32        \u001b[36m0.9787\u001b[0m        \u001b[32m1.1993\u001b[0m  0.0945\n",
      "     33        \u001b[36m0.8795\u001b[0m        \u001b[32m1.0955\u001b[0m  0.0989\n",
      "     34        \u001b[36m0.7976\u001b[0m        \u001b[32m1.0087\u001b[0m  0.0973\n",
      "     35        \u001b[36m0.7294\u001b[0m        \u001b[32m0.9357\u001b[0m  0.0966\n",
      "     36        \u001b[36m0.6721\u001b[0m        \u001b[32m0.8738\u001b[0m  0.0965\n",
      "     37        \u001b[36m0.6234\u001b[0m        \u001b[32m0.8209\u001b[0m  0.1053\n",
      "     38        \u001b[36m0.5816\u001b[0m        \u001b[32m0.7752\u001b[0m  0.0955\n",
      "     39        \u001b[36m0.5453\u001b[0m        \u001b[32m0.7351\u001b[0m  0.0925\n",
      "     40        \u001b[36m0.5135\u001b[0m        \u001b[32m0.6992\u001b[0m  0.0918\n",
      "     41        \u001b[36m0.4852\u001b[0m        \u001b[32m0.6665\u001b[0m  0.0922\n",
      "     42        \u001b[36m0.4597\u001b[0m        \u001b[32m0.6361\u001b[0m  0.1004\n",
      "     43        \u001b[36m0.4365\u001b[0m        \u001b[32m0.6073\u001b[0m  0.0962\n",
      "     44        \u001b[36m0.4151\u001b[0m        \u001b[32m0.5796\u001b[0m  0.0906\n",
      "     45        \u001b[36m0.3953\u001b[0m        \u001b[32m0.5527\u001b[0m  0.0924\n",
      "     46        \u001b[36m0.3768\u001b[0m        \u001b[32m0.5265\u001b[0m  0.0914\n",
      "     47        \u001b[36m0.3595\u001b[0m        \u001b[32m0.5009\u001b[0m  0.0902\n",
      "     48        \u001b[36m0.3432\u001b[0m        \u001b[32m0.4761\u001b[0m  0.0975\n",
      "     49        \u001b[36m0.3279\u001b[0m        \u001b[32m0.4521\u001b[0m  0.1594\n",
      "     50        \u001b[36m0.3136\u001b[0m        \u001b[32m0.4292\u001b[0m  0.0904\n",
      "     51        \u001b[36m0.3003\u001b[0m        \u001b[32m0.4077\u001b[0m  0.0892\n",
      "     52        \u001b[36m0.2879\u001b[0m        \u001b[32m0.3878\u001b[0m  0.1023\n",
      "     53        \u001b[36m0.2765\u001b[0m        \u001b[32m0.3695\u001b[0m  0.1151\n",
      "     54        \u001b[36m0.2660\u001b[0m        \u001b[32m0.3528\u001b[0m  0.0912\n",
      "     55        \u001b[36m0.2564\u001b[0m        \u001b[32m0.3378\u001b[0m  0.1326\n",
      "     56        \u001b[36m0.2476\u001b[0m        \u001b[32m0.3243\u001b[0m  0.1053\n",
      "     57        \u001b[36m0.2396\u001b[0m        \u001b[32m0.3120\u001b[0m  0.0904\n",
      "     58        \u001b[36m0.2322\u001b[0m        \u001b[32m0.3009\u001b[0m  0.0982\n",
      "     59        \u001b[36m0.2254\u001b[0m        \u001b[32m0.2908\u001b[0m  0.0931\n",
      "     60        \u001b[36m0.2191\u001b[0m        \u001b[32m0.2815\u001b[0m  0.0917\n",
      "     61        \u001b[36m0.2133\u001b[0m        \u001b[32m0.2730\u001b[0m  0.0942\n",
      "     62        \u001b[36m0.2080\u001b[0m        \u001b[32m0.2651\u001b[0m  0.1032\n",
      "     63        \u001b[36m0.2030\u001b[0m        \u001b[32m0.2579\u001b[0m  0.0932\n",
      "     64        \u001b[36m0.1983\u001b[0m        \u001b[32m0.2511\u001b[0m  0.0934\n",
      "     65        \u001b[36m0.1940\u001b[0m        \u001b[32m0.2448\u001b[0m  0.0942\n",
      "     66        \u001b[36m0.1900\u001b[0m        \u001b[32m0.2389\u001b[0m  0.0914\n",
      "     67        \u001b[36m0.1862\u001b[0m        \u001b[32m0.2334\u001b[0m  0.0899\n",
      "     68        \u001b[36m0.1826\u001b[0m        \u001b[32m0.2282\u001b[0m  0.0943\n",
      "     69        \u001b[36m0.1793\u001b[0m        \u001b[32m0.2233\u001b[0m  0.0915\n",
      "     70        \u001b[36m0.1761\u001b[0m        \u001b[32m0.2188\u001b[0m  0.0925\n",
      "     71        \u001b[36m0.1732\u001b[0m        \u001b[32m0.2145\u001b[0m  0.0923\n",
      "     72        \u001b[36m0.1704\u001b[0m        \u001b[32m0.2104\u001b[0m  0.0926\n",
      "     73        \u001b[36m0.1677\u001b[0m        \u001b[32m0.2066\u001b[0m  0.0974\n",
      "     74        \u001b[36m0.1652\u001b[0m        \u001b[32m0.2030\u001b[0m  0.1064\n",
      "     75        \u001b[36m0.1629\u001b[0m        \u001b[32m0.1996\u001b[0m  0.0914\n",
      "     76        \u001b[36m0.1606\u001b[0m        \u001b[32m0.1963\u001b[0m  0.0904\n",
      "     77        \u001b[36m0.1585\u001b[0m        \u001b[32m0.1933\u001b[0m  0.0916\n",
      "     78        \u001b[36m0.1565\u001b[0m        \u001b[32m0.1904\u001b[0m  0.0937\n",
      "     79        \u001b[36m0.1546\u001b[0m        \u001b[32m0.1876\u001b[0m  0.0922\n",
      "     80        \u001b[36m0.1528\u001b[0m        \u001b[32m0.1850\u001b[0m  0.0966\n",
      "     81        \u001b[36m0.1511\u001b[0m        \u001b[32m0.1825\u001b[0m  0.0924\n",
      "     82        \u001b[36m0.1494\u001b[0m        \u001b[32m0.1802\u001b[0m  0.0904\n",
      "     83        \u001b[36m0.1479\u001b[0m        \u001b[32m0.1780\u001b[0m  0.0994\n",
      "     84        \u001b[36m0.1464\u001b[0m        \u001b[32m0.1759\u001b[0m  0.1005\n",
      "     85        \u001b[36m0.1450\u001b[0m        \u001b[32m0.1739\u001b[0m  0.0951\n",
      "     86        \u001b[36m0.1437\u001b[0m        \u001b[32m0.1720\u001b[0m  0.0951\n",
      "     87        \u001b[36m0.1424\u001b[0m        \u001b[32m0.1702\u001b[0m  0.0954\n",
      "     88        \u001b[36m0.1412\u001b[0m        \u001b[32m0.1685\u001b[0m  0.0925\n",
      "     89        \u001b[36m0.1401\u001b[0m        \u001b[32m0.1668\u001b[0m  0.0923\n",
      "     90        \u001b[36m0.1390\u001b[0m        \u001b[32m0.1653\u001b[0m  0.0913\n",
      "     91        \u001b[36m0.1380\u001b[0m        \u001b[32m0.1638\u001b[0m  0.0914\n",
      "     92        \u001b[36m0.1371\u001b[0m        \u001b[32m0.1624\u001b[0m  0.1814\n",
      "     93        \u001b[36m0.1361\u001b[0m        \u001b[32m0.1610\u001b[0m  0.0993\n",
      "     94        \u001b[36m0.1353\u001b[0m        \u001b[32m0.1597\u001b[0m  0.0913\n",
      "     95        \u001b[36m0.1345\u001b[0m        \u001b[32m0.1583\u001b[0m  0.0925\n",
      "     96        \u001b[36m0.1337\u001b[0m        \u001b[32m0.1570\u001b[0m  0.0912\n",
      "     97        \u001b[36m0.1329\u001b[0m        \u001b[32m0.1557\u001b[0m  0.0913\n",
      "     98        \u001b[36m0.1322\u001b[0m        \u001b[32m0.1544\u001b[0m  0.1083\n",
      "     99        \u001b[36m0.1315\u001b[0m        \u001b[32m0.1531\u001b[0m  0.0883\n",
      "    100        \u001b[36m0.1308\u001b[0m        \u001b[32m0.1518\u001b[0m  0.0914\n",
      "    101        \u001b[36m0.1302\u001b[0m        \u001b[32m0.1505\u001b[0m  0.0905\n",
      "    102        \u001b[36m0.1295\u001b[0m        \u001b[32m0.1492\u001b[0m  0.0913\n",
      "    103        \u001b[36m0.1289\u001b[0m        \u001b[32m0.1480\u001b[0m  0.0924\n",
      "    104        \u001b[36m0.1283\u001b[0m        \u001b[32m0.1467\u001b[0m  0.0948\n",
      "    105        \u001b[36m0.1276\u001b[0m        \u001b[32m0.1455\u001b[0m  0.0905\n",
      "    106        \u001b[36m0.1270\u001b[0m        \u001b[32m0.1444\u001b[0m  0.0914\n",
      "    107        \u001b[36m0.1264\u001b[0m        \u001b[32m0.1433\u001b[0m  0.0922\n",
      "    108        \u001b[36m0.1258\u001b[0m        \u001b[32m0.1424\u001b[0m  0.0914\n",
      "    109        \u001b[36m0.1252\u001b[0m        \u001b[32m0.1415\u001b[0m  0.0922\n",
      "    110        \u001b[36m0.1245\u001b[0m        \u001b[32m0.1407\u001b[0m  0.0904\n",
      "    111        \u001b[36m0.1239\u001b[0m        \u001b[32m0.1400\u001b[0m  0.0933\n",
      "    112        \u001b[36m0.1233\u001b[0m        \u001b[32m0.1394\u001b[0m  0.0903\n",
      "    113        \u001b[36m0.1227\u001b[0m        \u001b[32m0.1389\u001b[0m  0.0915\n",
      "    114        \u001b[36m0.1221\u001b[0m        \u001b[32m0.1384\u001b[0m  0.0935\n",
      "    115        \u001b[36m0.1215\u001b[0m        \u001b[32m0.1379\u001b[0m  0.0931\n",
      "    116        \u001b[36m0.1209\u001b[0m        \u001b[32m0.1375\u001b[0m  0.0893\n",
      "    117        \u001b[36m0.1204\u001b[0m        \u001b[32m0.1371\u001b[0m  0.0914\n",
      "    118        \u001b[36m0.1199\u001b[0m        \u001b[32m0.1367\u001b[0m  0.0905\n",
      "    119        \u001b[36m0.1194\u001b[0m        \u001b[32m0.1362\u001b[0m  0.0923\n",
      "    120        \u001b[36m0.1189\u001b[0m        \u001b[32m0.1358\u001b[0m  0.0923\n",
      "    121        \u001b[36m0.1184\u001b[0m        \u001b[32m0.1354\u001b[0m  0.0913\n",
      "    122        \u001b[36m0.1180\u001b[0m        \u001b[32m0.1349\u001b[0m  0.1094\n",
      "    123        \u001b[36m0.1176\u001b[0m        \u001b[32m0.1344\u001b[0m  0.0914\n",
      "    124        \u001b[36m0.1172\u001b[0m        \u001b[32m0.1339\u001b[0m  0.0945\n",
      "    125        \u001b[36m0.1169\u001b[0m        \u001b[32m0.1334\u001b[0m  0.0962\n",
      "    126        \u001b[36m0.1166\u001b[0m        \u001b[32m0.1329\u001b[0m  0.0942\n",
      "    127        \u001b[36m0.1163\u001b[0m        \u001b[32m0.1324\u001b[0m  0.0914\n",
      "    128        \u001b[36m0.1160\u001b[0m        \u001b[32m0.1319\u001b[0m  0.0912\n",
      "    129        \u001b[36m0.1157\u001b[0m        \u001b[32m0.1314\u001b[0m  0.0946\n",
      "    130        \u001b[36m0.1155\u001b[0m        \u001b[32m0.1308\u001b[0m  0.0921\n",
      "    131        \u001b[36m0.1153\u001b[0m        \u001b[32m0.1303\u001b[0m  0.0921\n",
      "    132        \u001b[36m0.1151\u001b[0m        \u001b[32m0.1298\u001b[0m  0.0924\n",
      "    133        \u001b[36m0.1149\u001b[0m        \u001b[32m0.1293\u001b[0m  0.0904\n",
      "    134        \u001b[36m0.1148\u001b[0m        \u001b[32m0.1288\u001b[0m  0.0913\n",
      "    135        \u001b[36m0.1146\u001b[0m        \u001b[32m0.1283\u001b[0m  0.0915\n",
      "    136        \u001b[36m0.1145\u001b[0m        \u001b[32m0.1279\u001b[0m  0.0993\n",
      "    137        \u001b[36m0.1144\u001b[0m        \u001b[32m0.1274\u001b[0m  0.0943\n",
      "    138        \u001b[36m0.1143\u001b[0m        \u001b[32m0.1270\u001b[0m  0.0894\n",
      "    139        \u001b[36m0.1142\u001b[0m        \u001b[32m0.1266\u001b[0m  0.0903\n",
      "    140        \u001b[36m0.1141\u001b[0m        \u001b[32m0.1261\u001b[0m  0.0914\n",
      "    141        \u001b[36m0.1141\u001b[0m        \u001b[32m0.1257\u001b[0m  0.0913\n",
      "    142        \u001b[36m0.1140\u001b[0m        \u001b[32m0.1254\u001b[0m  0.0915\n",
      "    143        \u001b[36m0.1140\u001b[0m        \u001b[32m0.1250\u001b[0m  0.0932\n",
      "    144        \u001b[36m0.1139\u001b[0m        \u001b[32m0.1246\u001b[0m  0.0925\n",
      "    145        \u001b[36m0.1139\u001b[0m        \u001b[32m0.1243\u001b[0m  0.0916\n",
      "    146        \u001b[36m0.1139\u001b[0m        \u001b[32m0.1240\u001b[0m  0.1082\n",
      "    147        \u001b[36m0.1139\u001b[0m        \u001b[32m0.1237\u001b[0m  0.0924\n",
      "    148        \u001b[36m0.1139\u001b[0m        \u001b[32m0.1234\u001b[0m  0.0954\n",
      "    149        0.1139        \u001b[32m0.1231\u001b[0m  0.0902\n",
      "    150        0.1139        \u001b[32m0.1228\u001b[0m  0.0915\n",
      "    151        0.1139        \u001b[32m0.1226\u001b[0m  0.0913\n",
      "    152        0.1139        \u001b[32m0.1224\u001b[0m  0.0901\n",
      "    153        0.1140        \u001b[32m0.1221\u001b[0m  0.0927\n",
      "    154        0.1140        \u001b[32m0.1219\u001b[0m  0.0933\n",
      "    155        0.1141        \u001b[32m0.1218\u001b[0m  0.0923\n",
      "    156        0.1141        \u001b[32m0.1216\u001b[0m  0.1989\n",
      "    157        0.1142        \u001b[32m0.1214\u001b[0m  0.0943\n",
      "    158        0.1142        \u001b[32m0.1213\u001b[0m  0.0933\n",
      "    159        0.1143        \u001b[32m0.1212\u001b[0m  0.0924\n",
      "    160        0.1144        \u001b[32m0.1211\u001b[0m  0.0914\n",
      "    161        0.1144        \u001b[32m0.1210\u001b[0m  0.0933\n",
      "    162        0.1145        \u001b[32m0.1210\u001b[0m  0.0916\n",
      "    163        0.1146        \u001b[32m0.1209\u001b[0m  0.0934\n",
      "    164        0.1146        \u001b[32m0.1209\u001b[0m  0.0924\n",
      "    165        0.1147        \u001b[32m0.1209\u001b[0m  0.0937\n",
      "    166        0.1148        \u001b[32m0.1209\u001b[0m  0.0919\n",
      "    167        0.1148        0.1209  0.0933\n",
      "    168        0.1149        0.1209  0.0913\n",
      "    169        0.1150        0.1209  0.0914\n",
      "    170        0.1150        0.1209  0.0903\n",
      "    171        0.1151        0.1209  0.1086\n",
      "    172        0.1151        0.1209  0.0914\n",
      "    173        0.1151        0.1210  0.0913\n",
      "    174        0.1151        0.1210  0.0906\n",
      "    175        0.1151        0.1210  0.0924\n",
      "    176        0.1151        0.1210  0.0929\n",
      "    177        0.1151        0.1211  0.0913\n",
      "    178        0.1150        0.1211  0.0904\n",
      "    179        0.1150        0.1211  0.0913\n",
      "    180        0.1149        0.1211  0.0905\n",
      "    181        0.1149        0.1212  0.0946\n",
      "    182        0.1148        0.1212  0.0902\n",
      "    183        0.1147        0.1212  0.0908\n",
      "    184        0.1146        0.1213  0.0926\n",
      "    185        0.1145        0.1213  0.0912\n",
      "    186        0.1144        0.1214  0.0926\n",
      "    187        0.1142        0.1214  0.0905\n",
      "    188        0.1141        0.1215  0.0934\n",
      "    189        0.1140        0.1216  0.0923\n",
      "    190        \u001b[36m0.1138\u001b[0m        0.1216  0.0914\n",
      "    191        \u001b[36m0.1137\u001b[0m        0.1217  0.0906\n",
      "    192        \u001b[36m0.1136\u001b[0m        0.1218  0.0935\n",
      "    193        \u001b[36m0.1134\u001b[0m        0.1219  0.0902\n",
      "    194        \u001b[36m0.1133\u001b[0m        0.1221  0.0915\n",
      "    195        \u001b[36m0.1131\u001b[0m        0.1222  0.0934\n",
      "    196        \u001b[36m0.1130\u001b[0m        0.1223  0.0905\n",
      "    197        \u001b[36m0.1129\u001b[0m        0.1225  0.0921\n",
      "    198        \u001b[36m0.1127\u001b[0m        0.1226  0.1071\n",
      "    199        \u001b[36m0.1126\u001b[0m        0.1227  0.0916\n",
      "    200        \u001b[36m0.1125\u001b[0m        0.1229  0.0913\n",
      "    201        \u001b[36m0.1123\u001b[0m        0.1230  0.0903\n",
      "    202        \u001b[36m0.1122\u001b[0m        0.1232  0.0904\n",
      "    203        \u001b[36m0.1121\u001b[0m        0.1233  0.0938\n",
      "    204        \u001b[36m0.1120\u001b[0m        0.1234  0.1047\n",
      "    205        \u001b[36m0.1119\u001b[0m        0.1236  0.0953\n",
      "    206        \u001b[36m0.1118\u001b[0m        0.1237  0.0923\n",
      "    207        \u001b[36m0.1118\u001b[0m        0.1238  0.0916\n",
      "    208        \u001b[36m0.1117\u001b[0m        0.1239  0.0913\n",
      "    209        \u001b[36m0.1116\u001b[0m        0.1240  0.0934\n",
      "    210        \u001b[36m0.1116\u001b[0m        0.1241  0.0905\n",
      "    211        \u001b[36m0.1115\u001b[0m        0.1242  0.0912\n",
      "    212        \u001b[36m0.1115\u001b[0m        0.1243  0.0941\n",
      "    213        \u001b[36m0.1114\u001b[0m        0.1243  0.0923\n",
      "    214        \u001b[36m0.1114\u001b[0m        0.1244  0.0913\n",
      "Stopping since valid_loss has not improved in the last 50 epochs.\n",
      "→ Ensemble NNet | Train R²: 0.9989 | Test R²: 0.5869 | Test MAE: 4.6287\n",
      "Best NN params: {'TorchNN__optimizer__lr': 0.02, 'TorchNN__module__num_layers': 2, 'TorchNN__module__hidden_units': 8, 'TorchNN__module__dropout': 0, 'TorchNN__module__activation': <class 'torch.nn.modules.activation.Sigmoid'>, 'TorchNN__max_epochs': 300, 'TorchNN__batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------------\n",
    "\"\"\"The idea is to let train nystroem and XGB as good as possible, then feed the results to a small neural network, alongside the most relevant features.\"\"\"\n",
    "def create_nnet_input(X: pd.DataFrame, **model_preds) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create input features for the neural network by combining selected features and model predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Input feature DataFrame\n",
    "    model_preds (dict): Dictionary of model predictions with model names as keys and prediction arrays as values\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing selected features and model predictions\n",
    "    \"\"\"\n",
    "    primary_model_features = [pd.Series(pred, name=model_name, index=X.index) for model_name, pred in model_preds.items()]\n",
    "    X = pd.concat([X] + primary_model_features, axis=1)\n",
    "    assert (isinstance(X, pd.DataFrame)), \"Output is not a DataFrame\"\n",
    "    return X\n",
    "\n",
    "#create the input features for the neural network\n",
    "primary_model_preds_train = {\n",
    "    \"XGB_Pred\": y_train_pred_xgb,\n",
    "    \"NYS_Pred\": y_train_pred_nys,\n",
    "    \"RndFrst\": y_train_pred_rndm_frst,\n",
    "    \"SVR\": y_train_pred_svr,\n",
    "}\n",
    "primary_model_preds = {\n",
    "    \"XGB_Pred\": y_pred_xgb,\n",
    "    \"NYS_Pred\": y_pred_nys,\n",
    "    \"RndFrst\": y_pred_rndm_frst,\n",
    "    \"SVR\": y_pred_svr,\n",
    "}\n",
    "\n",
    "print(f\"considering {len(features_nnet)} primary features for NN\")\n",
    "nnet_inp = create_nnet_input(X_train[features_nnet], **primary_model_preds_train)\n",
    "nnet_val = create_nnet_input(X_val[features_nnet], **primary_model_preds)\n",
    "#convert to skorch compatible format\n",
    "nnet_inp = nnet_inp.astype(np.float32).values\n",
    "nnet_val = nnet_val.astype(np.float32).values\n",
    "y_train = y_train.astype(np.float32).values\n",
    "y_val = y_val.astype(np.float32).values\n",
    "\n",
    "#now we do a hyperparam search for the neural network\n",
    "input_layer_dim = int(nnet_inp.shape[1])\n",
    "print(\"Input layer dim for NN:\", input_layer_dim)\n",
    "nn_param_grid = {\n",
    "    \"TorchNN\": {\n",
    "            \"model\": NeuralNetRegressor(\n",
    "                module=MLP,\n",
    "                module__input_dim=input_layer_dim,  # Correct input dim\n",
    "                criterion=nn.MSELoss,\n",
    "                optimizer=optim.Adam,\n",
    "                verbose=1,\n",
    "                callbacks=[EarlyStopping(monitor='valid_loss', patience=50, threshold=0.0001)]\n",
    "\n",
    "            ),\n",
    "            \"params\": {\n",
    "                \"TorchNN__module__hidden_units\": [input_layer_dim * 2, input_layer_dim * 4],\n",
    "                \"TorchNN__module__num_layers\": [2, 3, 5],\n",
    "                \"TorchNN__module__dropout\": [0, 0.1, 0.3],\n",
    "                \"TorchNN__module__activation\" : [nn.Sigmoid,nn.ReLU],\n",
    "                \"TorchNN__optimizer__lr\": [0.02, 0.01],\n",
    "                \"TorchNN__batch_size\": [32],\n",
    "                \"TorchNN__max_epochs\": [300]\n",
    "            },\n",
    "        },\n",
    "}\n",
    "\n",
    "nn_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"TorchNN\", nn_param_grid[\"TorchNN\"][\"model\"])\n",
    "])\n",
    "\n",
    "param_grid = nn_param_grid[\"TorchNN\"][\"params\"]\n",
    "\n",
    "nn_gs = RandomizedSearchCV(\n",
    "    estimator=nn_pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    n_iter=10,\n",
    "    random_state=42,\n",
    "\n",
    ")\n",
    "\n",
    "nn_gs.fit(nnet_inp, y_train)\n",
    "y_pred_train_ensemble = nn_gs.predict(nnet_inp)\n",
    "y_pred_val_ensemble = nn_gs.predict(nnet_val)\n",
    "\n",
    "print(f\"→ Ensemble NNet | Train R²: {r2_score(y_train, y_pred_train_ensemble):.4f} | Test R²: {r2_score(y_val, y_pred_val_ensemble):.4f} | Test MAE: {mean_absolute_error(y_val, y_pred_val_ensemble):.4f}\")\n",
    "print(\"Best NN params:\", nn_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_training(model):\n",
    "        #TODO for ensemble\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b33e9",
   "metadata": {},
   "source": [
    "# Train the model once again on the full dataset for the public test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafaaf37",
   "metadata": {},
   "source": [
    "# Save the test results for online evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14764918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__subsample': 0.6, 'model__reg_alpha': 0.6, 'model__n_estimators': 600, 'model__max_depth': 9, 'model__learning_rate': 0.02, 'model__gamma': 1, 'model__colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [10:38:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"model__colsample_bytree\", \"model__gamma\", \"model__learning_rate\", \"model__max_depth\", \"model__n_estimators\", \"model__reg_alpha\", \"model__subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to results/test_xgboost_20251104_103859.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kaggle competitions submit -c eth-aml-2025-project-1 -f submission.csv -m \"Message\"'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def save_test_predictions(model, model_name)-> Path:\n",
    "    model = final_training(model)\n",
    "    save_path = results_dir / f\"test_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "    y_test_pred = model.predict(X_test[X_selected.columns])\n",
    "    results_df = pd.DataFrame(y_test_pred, index=X_test.index, columns=[\"y\"])\n",
    "    assert (results_df.shape == (776, 1)), f\"Unexpected shape: {results_df.shape}\"\n",
    "    results_df.to_csv(save_path)\n",
    "    print(f\"Test predictions saved to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "file = save_test_predictions(XGBRegressor, \"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5ed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.93k/9.93k [00:00<00:00, 25.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to ETH AML 2025 Project 1None None\n"
     ]
    }
   ],
   "source": [
    "#upload to kaggle\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#submit predictions\n",
    "def submit_predictions(file:Path, message:str) -> None:\n",
    "    cmd = f'kaggle competitions submit -c eth-aml-2025-project-1 -f {file} -m \"{message}\"'\n",
    "    res = subprocess.run(cmd, shell=True, check=True)\n",
    "    print(res.stdout, res.stderr)\n",
    "\n",
    "submit_msg = input(\"Enter submission message: \")\n",
    "submit_predictions(file, submit_msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LRF)",
   "language": "python",
   "name": "lrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
