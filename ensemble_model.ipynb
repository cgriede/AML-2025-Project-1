{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbbf2c3",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7806c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (1212, 832), y_train.shape (1212, 1), X_test.shape (776, 832)\n",
      "              x0            x1           x2             x3          x4  \\\n",
      "id                                                                       \n",
      "0   14168.823171  10514.380717  3316.149698   94230.695124  102.386606   \n",
      "1   17757.037554           NaN  4101.016273   92959.527633         NaN   \n",
      "2   14226.656663  11029.642499          NaN  124055.600561  100.542483   \n",
      "3    8766.012436   7384.202998  2147.308418  100157.719990  104.855061   \n",
      "4   13801.016418  13269.493652  3408.316953   92048.527786  103.759758   \n",
      "\n",
      "            x5            x6            x7            x8         x9  ...  \\\n",
      "id                                                                   ...   \n",
      "0    92.677127  11108.748199  10866.505510  10837.622093  10.227734  ...   \n",
      "1    99.855168  10013.959449  10826.607494  10076.101597  11.436970  ...   \n",
      "2    92.860892           NaN  10492.342868           NaN  10.810076  ...   \n",
      "3   101.929026  10050.049932  10499.521099  10525.030989  10.092109  ...   \n",
      "4    95.789235   9667.353978  10750.783106  10618.800750  12.006773  ...   \n",
      "\n",
      "            x822          x823        x824        x825        x826  \\\n",
      "id                                                                   \n",
      "0            NaN  12352.094085  846.014651  105.132144  102.112809   \n",
      "1            NaN  16198.071494  776.084467  106.385590  103.472030   \n",
      "2   10329.704431  13976.063780  737.040332  103.671234  109.458246   \n",
      "3   10008.251395   6212.127347  329.044233  105.084488  104.858546   \n",
      "4   10095.782015  13772.061493         NaN         NaN  100.369834   \n",
      "\n",
      "           x827      x828         x829         x830          x831  \n",
      "id                                                                 \n",
      "0   2090.004260  2.691845  1234.374109  1000.784475   9285.751272  \n",
      "1   2474.051881  2.287976          NaN  1012.626705  11750.284764  \n",
      "2   2656.083281  2.843706   888.353607  1048.810385   9553.922728  \n",
      "3   1097.785204  2.732257   927.752967  1048.357330           NaN  \n",
      "4   2693.053231  2.702908  1471.354073  1071.284484   9423.533063  \n",
      "\n",
      "[5 rows x 832 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "X_test = data_dir / \"X_test.csv\"\n",
    "y_train = data_dir / \"y_train.csv\"\n",
    "X_train = data_dir / \"X_train.csv\"\n",
    "\n",
    "X_test = pd.read_csv(X_test, index_col=\"id\")\n",
    "X_test.index = X_test.index.astype(int)\n",
    "\n",
    "y_train = pd.read_csv(y_train, index_col=\"id\")\n",
    "y_train.index = y_train.index.astype(int)\n",
    "\n",
    "X_train = pd.read_csv(X_train, index_col=\"id\")\n",
    "X_train.index = X_train.index.astype(int)\n",
    "\n",
    "print(f\"X_train.shape {X_train.shape}, y_train.shape {y_train.shape}, X_test.shape {X_test.shape}\")\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7d5437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#have a look at the data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train.values, bins=50)\n",
    "plt.xlabel(\"Target Variable\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Target Variable\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b28cc",
   "metadata": {},
   "source": [
    "# 1. imputing missing values\n",
    "\n",
    "First we impute nan values with the mean for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5a23ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan-values: 0\n",
      "              x0            x1           x2             x3          x4  \\\n",
      "id                                                                       \n",
      "0   14168.823171  10514.380717  3316.149698   94230.695124  102.386606   \n",
      "1   17757.037554  10950.160761  4101.016273   92959.527633  105.070358   \n",
      "2   14226.656663  11029.642499  3430.837498  124055.600561  100.542483   \n",
      "3    8766.012436   7384.202998  2147.308418  100157.719990  104.855061   \n",
      "4   13801.016418  13269.493652  3408.316953   92048.527786  103.759758   \n",
      "\n",
      "            x5            x6            x7            x8         x9  ...  \\\n",
      "id                                                                   ...   \n",
      "0    92.677127  11108.748199  10866.505510  10837.622093  10.227734  ...   \n",
      "1    99.855168  10013.959449  10826.607494  10076.101597  11.436970  ...   \n",
      "2    92.860892   9983.055476  10492.342868  10495.835570  10.810076  ...   \n",
      "3   101.929026  10050.049932  10499.521099  10525.030989  10.092109  ...   \n",
      "4    95.789235   9667.353978  10750.783106  10618.800750  12.006773  ...   \n",
      "\n",
      "            x822          x823        x824        x825        x826  \\\n",
      "id                                                                   \n",
      "0   10069.191241  12352.094085  846.014651  105.132144  102.112809   \n",
      "1   10069.191241  16198.071494  776.084467  106.385590  103.472030   \n",
      "2   10329.704431  13976.063780  737.040332  103.671234  109.458246   \n",
      "3   10008.251395   6212.127347  329.044233  105.084488  104.858546   \n",
      "4   10095.782015  13772.061493  812.316152  104.968652  100.369834   \n",
      "\n",
      "           x827      x828         x829         x830          x831  \n",
      "id                                                                 \n",
      "0   2090.004260  2.691845  1234.374109  1000.784475   9285.751272  \n",
      "1   2474.051881  2.287976  1359.981226  1012.626705  11750.284764  \n",
      "2   2656.083281  2.843706   888.353607  1048.810385   9553.922728  \n",
      "3   1097.785204  2.732257   927.752967  1048.357330   9981.085085  \n",
      "4   2693.053231  2.702908  1471.354073  1071.284484   9423.533063  \n",
      "\n",
      "[5 rows x 832 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train.fillna(pd.Series(X_train.mean()), inplace=True)\n",
    "print(f\"Nan-values: {X_train.isna().sum().sum()}\")\n",
    "print(X_train.head())\n",
    "X_test.fillna(pd.Series(X_test.mean()), inplace=True)\n",
    "y_train.fillna(pd.Series(y_train.mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d4ae2",
   "metadata": {},
   "source": [
    "# 2. Outlier detection\n",
    "Next we build an outlier detection model to classify samples as outliers and eventually remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2064cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier indices: {np.int64(3), np.int64(515), np.int64(645), np.int64(900), np.int64(1031), np.int64(648), np.int64(906), np.int64(397), np.int64(917), np.int64(278), np.int64(24), np.int64(1048), np.int64(30), np.int64(416), np.int64(1056), np.int64(1058), np.int64(805), np.int64(167), np.int64(297), np.int64(681), np.int64(428), np.int64(306), np.int64(821), np.int64(694), np.int64(697), np.int64(953), np.int64(573), np.int64(190), np.int64(575), np.int64(64), np.int64(321), np.int64(448), np.int64(837), np.int64(70), np.int64(583), np.int64(75), np.int64(460), np.int64(972), np.int64(1102), np.int64(335), np.int64(474), np.int64(91), np.int64(731), np.int64(860), np.int64(991), np.int64(480), np.int64(225), np.int64(1120), np.int64(100), np.int64(229), np.int64(740), np.int64(231), np.int64(234), np.int64(362), np.int64(1137), np.int64(114), np.int64(882), np.int64(502), np.int64(1144), np.int64(506), np.int64(1150)}\n",
      "Number of outliers detected: 61\n"
     ]
    }
   ],
   "source": [
    "#outlier detection using isolation forest and guessing the contamination to be 5%\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "outlier_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "outlier_model.fit(X_train)\n",
    "outlier_preds = outlier_model.predict(X_train)\n",
    "isoforest_outlier_indices = set(np.where(outlier_preds == -1)[0])\n",
    "\n",
    "print(\"Outlier indices:\", isoforest_outlier_indices)\n",
    "\n",
    "print(f\"Number of outliers detected: {len(isoforest_outlier_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac8194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Outlier indices: [  16   30   32   56   70   72   88   91  142  168  180  207  211  214\n",
      "  245  278  313  321  334  352  397  403  404  431  518  537  545  553\n",
      "  570  574  583  616  622  631  633  635  681  694  731  732  741  755\n",
      "  771  805  806  808  836  872  882  888  899  900  906  917  949  953\n",
      "  972  990  994 1001 1038 1066 1081 1087 1102 1132 1135 1144 1205]\n",
      "Number of outliers detected by SVM: 69\n",
      "Number of common outliers detected by both methods: 19\n",
      "Common outlier indices: {np.int64(900), np.int64(906), np.int64(397), np.int64(917), np.int64(278), np.int64(30), np.int64(805), np.int64(681), np.int64(694), np.int64(953), np.int64(321), np.int64(70), np.int64(583), np.int64(972), np.int64(1102), np.int64(731), np.int64(91), np.int64(882), np.int64(1144)}\n"
     ]
    }
   ],
   "source": [
    "# outlier detection using OneClassSVM\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "svm_model = OneClassSVM(nu=0.01, kernel=\"rbf\", gamma=\"scale\")\n",
    "svm_model.fit(X_train_scaled)\n",
    "svm_preds = svm_model.predict(X_train_scaled)\n",
    "\n",
    "print(\"SVM Outlier indices:\", np.where(svm_preds == -1)[0])\n",
    "\n",
    "svm_outlier_indices = set(np.where(svm_preds == -1)[0])\n",
    "\n",
    "print(f\"Number of outliers detected by SVM: {np.sum(svm_preds == -1)}\")\n",
    "\n",
    "# Comparing the two methods\n",
    "common_outliers = isoforest_outlier_indices & svm_outlier_indices\n",
    "print(f\"Number of common outliers detected by both methods: {len(common_outliers)}\")\n",
    "print(\"Common outlier indices:\", common_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f5a894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned X_train shape: (1193, 832), Cleaned y_train shape: (1193, 1)\n"
     ]
    }
   ],
   "source": [
    "#removing outliers from the training data\n",
    "X_train_cleaned = X_train.drop(index=X_train.index[list(common_outliers)])\n",
    "y_train_cleaned = y_train.drop(index=y_train.index[list(common_outliers)])\n",
    "print(f\"Cleaned X_train shape: {X_train_cleaned.shape}, Cleaned y_train shape: {y_train_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4d015",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "this step is harder than imagined, a new idea came to my mind:\n",
    "\n",
    "1: select a subset of features randomly, train and score the predictions, as well as the features used\n",
    "\n",
    "2: do it for another subset\n",
    "\n",
    "3: mark the features which occur in higher scored subsets and use these for the final training (Monte Carlo aproach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebc46c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features to drop: ['x69', 'x132', 'x151', 'x202', 'x230', 'x231', 'x240', 'x250', 'x278', 'x288', 'x316', 'x317', 'x333', 'x334', 'x348', 'x374', 'x392', 'x414', 'x442', 'x479', 'x502', 'x505', 'x562', 'x568', 'x579', 'x610', 'x614', 'x634', 'x668', 'x675', 'x698', 'x715', 'x727', 'x774', 'x780', 'x790']\n",
      "Number of features to drop: 36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def correlation_matrix_reduction(df, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features (one from each pair).\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input feature DataFrame\n",
    "    threshold (float): Correlation threshold\n",
    "    \n",
    "    Returns:\n",
    "    list: Features to drop\n",
    "    \"\"\"\n",
    "    # Compute absolute correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "    \n",
    "    # Create upper triangle mask\n",
    "    mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    upper = corr_matrix.where(mask)\n",
    "    \n",
    "    # Find columns with any correlation > threshold in upper triangle\n",
    "    to_drop = [col for col in upper.columns if (upper[col] > threshold).any()]\n",
    "    \n",
    "    print(\"Highly correlated features to drop:\", to_drop)\n",
    "    print(f\"Number of features to drop: {len(to_drop)}\")\n",
    "    \n",
    "    return to_drop\n",
    "\n",
    "\n",
    "to_drop_corr = correlation_matrix_reduction(X_train_cleaned, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc2f948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low variance features to drop: ['x33', 'x67', 'x68', 'x104', 'x129', 'x147', 'x166', 'x179', 'x183', 'x185', 'x189', 'x195', 'x196', 'x207', 'x208', 'x229', 'x262', 'x271', 'x281', 'x289', 'x292', 'x295', 'x296', 'x305', 'x307', 'x336', 'x338', 'x363', 'x384', 'x385', 'x446', 'x450', 'x462', 'x466', 'x489', 'x492', 'x500', 'x519', 'x529', 'x530', 'x550', 'x556', 'x580', 'x583', 'x586', 'x593', 'x605', 'x620', 'x625', 'x628', 'x666', 'x679', 'x682', 'x709', 'x724', 'x755', 'x803', 'x808', 'x815']\n",
      "Number of low variance features to drop: 59\n",
      "Final features to drop: ['x392', 'x202', 'x278', 'x151', 'x230', 'x625', 'x492', 'x698', 'x385', 'x292', 'x675', 'x620', 'x614', 'x610', 'x724', 'x307', 'x271', 'x240', 'x808', 'x229', 'x462', 'x682', 'x580', 'x446', 'x183', 'x132', 'x489', 'x530', 'x208', 'x815', 'x262', 'x709', 'x466', 'x774', 'x668', 'x316', 'x505', 'x790', 'x67', 'x666', 'x562', 'x374', 'x336', 'x333', 'x338', 'x363', 'x281', 'x189', 'x519', 'x715', 'x780', 'x207', 'x195', 'x479', 'x442', 'x634', 'x104', 'x250', 'x166', 'x502', 'x803', 'x317', 'x185', 'x305', 'x529', 'x556', 'x583', 'x586', 'x755', 'x147', 'x593', 'x450', 'x129', 'x605', 'x550', 'x68', 'x289', 'x384', 'x33', 'x628', 'x296', 'x288', 'x69', 'x727', 'x579', 'x568', 'x196', 'x500', 'x679', 'x334', 'x179', 'x295', 'x348', 'x231', 'x414']\n"
     ]
    }
   ],
   "source": [
    "#remove low variance features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "def low_variance_reduction(threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    selector.fit(X_train_cleaned)\n",
    "    to_keep = X_train_cleaned.columns[selector.get_support()]\n",
    "    to_drop = [col for col in X_train_cleaned.columns if col not in to_keep]\n",
    "    print(\"Low variance features to drop:\", to_drop)\n",
    "    print(f\"Number of low variance features to drop: {len(to_drop)}\")\n",
    "    return to_drop\n",
    "to_drop_var = low_variance_reduction()\n",
    "\n",
    "final_to_drop = list(set(to_drop_corr) | set(to_drop_var))\n",
    "print(\"Final features to drop:\", final_to_drop)\n",
    "X_selected = X_train_cleaned.drop(columns=final_to_drop, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c62b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Train quick XGBoost model\n",
    "quick_model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "file = Path(\"results/feature_importances.csv\")\n",
    "\n",
    "if file.exists():\n",
    "    importances = pd.read_csv(file, index_col=0).squeeze()\n",
    "\n",
    "else:\n",
    "    quick_model.fit(X_selected, y_train_cleaned)\n",
    "\n",
    "    # Get permutation importance\n",
    "    result = permutation_importance(\n",
    "        quick_model, \n",
    "        X_selected, \n",
    "        y_train_cleaned, \n",
    "        n_repeats=5, \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    # Extract and rank features\n",
    "    importances = pd.Series(result.importances_mean, index=X_selected.columns)\n",
    "    #store  the importances\n",
    "    importances.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_features(top_n=50):    \n",
    "    top_features = importances.nlargest(top_n).index.tolist()\n",
    "    # Select top features\n",
    "    return top_features\n",
    "\n",
    "n_features_xgb = 70\n",
    "features_xgb = select_top_features(n_features_xgb)\n",
    "n_features_nnet = 5\n",
    "features_nnet = select_top_features(n_features_nnet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f0ae5",
   "metadata": {},
   "source": [
    "# Main Task, fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Define a simple MLP module\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, activation, hidden_units=128, num_layers=2, dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_units), activation(), nn.Dropout(dropout)]\n",
    "        for _ in range(1, num_layers):\n",
    "            layers.extend([nn.Linear(hidden_units, hidden_units), activation(), nn.Dropout(dropout)])\n",
    "        layers.append(nn.Linear(hidden_units, 1))  # Regression output\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c45a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 70 features\n",
      "\n",
      "Training XGBoost...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "→ XGBoost | CV R²: 0.5588 | Test R²: 0.5691 | Test MAE: 4.6589\n",
      "\n",
      "============================================================\n",
      "FINAL MODEL COMPARISON\n",
      "============================================================\n",
      "  Model  Best R² (CV)  Train R²  Test R²  Test MAE\n",
      "XGBoost        0.5588    0.9979   0.5691    4.6589\n",
      "                                                                                                                                                                            Best Params\n",
      "{'model__subsample': 0.55, 'model__reg_alpha': 0.5, 'model__n_estimators': 600, 'model__max_depth': 9, 'model__learning_rate': 0.02, 'model__gamma': 1, 'model__colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === MULTI-MODEL COMPARISON FOR BRAIN MRI AGE PREDICTION ===\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV, train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_absolute_error\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 1. Split (keep X_selected from RandomSubsetSelector) ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_selected, y_train_cleaned, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. CV Strategy ---\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "xgb_params ={\n",
    "    'model__subsample'       : 0.6,\n",
    "    'model__reg_alpha'       : 0.6,\n",
    "    'model__n_estimators'    : 600,\n",
    "    'model__max_depth'       : 9,\n",
    "    'model__learning_rate'   : 0.02,\n",
    "    'model__gamma'           : 1,\n",
    "    'model__colsample_bytree': 0.7,\n",
    "    }\n",
    "X_train_xgb = X_selected[features_xgb]\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "pipe_nys = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nystroem\", Nystroem(kernel=\"rbf\")),\n",
    "])\n",
    "\n",
    "param_grid_nys = {\n",
    "    \"nystroem__gamma\": np.logspace(-4, 1, 6),\n",
    "    \"nystroem__n_components\": [500, 800, 1000],\n",
    "}\n",
    "\n",
    "#TODO: train the nystroem model and get the best params\n",
    "gs_nys = RandomizedSearchCV(\n",
    "    pipe_nys, param_grid_nys, cv=cv, scoring=scorer, n_jobs=-1, verbose=1, n_iter=10,\n",
    ")\n",
    "gs_nys.fit(X_train, y_train)\n",
    "best_nys_params = gs_nys.best_params_\n",
    "print(\"Best Nystroem Params:\", best_nys_params)\n",
    "\n",
    "#TODO: define the neural network pipeline and param grid\n",
    "\"\"\"The idea is to let train nystroem and XGB as good as possible, then feed the results to a small neural network, alongside the most relevant features.\"\"\"\n",
    "\n",
    "pipe_nnet = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nystroem\", Nystroem(kernel=\"rbf\")),\n",
    "    (\"model\", MLPRegressor(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_training(model):\n",
    "    params = results_df[[\"Best Params\"]].iloc[0].values[0]\n",
    "    print(params)\n",
    "    model = model(**params)\n",
    "    model.fit(X_selected, y_train_cleaned)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/joblib/__init__.py\", line 120, in <module>\n",
      "    from .memory import MemorizedResult, Memory, expires_after, register_store_backend\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/joblib/memory.py\", line 11, in <module>\n",
      "    import asyncio\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/asyncio/__init__.py\", line 8, in <module>\n",
      "    from .base_events import *\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/asyncio/base_events.py\", line 34, in <module>\n",
      "    import ssl\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/ssl.py\", line 394, in <module>\n",
      "    class _ASN1Object(namedtuple(\"_ASN1Object\", \"nid shortname longname oid\")):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/ssl.py\", line 394, in _ASN1Object\n",
      "    class _ASN1Object(namedtuple(\"_ASN1Object\", \"nid shortname longname oid\")):\n",
      "    \n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     76\u001b[39m pipe = Pipeline(steps)\n\u001b[32m     78\u001b[39m gs = GridSearchCV(\n\u001b[32m     79\u001b[39m     pipe, config[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m], cv=cv, scoring=scorer, n_jobs=-\u001b[32m1\u001b[39m, verbose=\u001b[32m1\u001b[39m\n\u001b[32m     80\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m y_pred_train_final = xgb.predict(X_train) + gs.predict(X_train).flatten()\n\u001b[32m     84\u001b[39m y_pred_t_final = xgb.predict(X_t) + gs.predict(X_t).flatten()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:2071\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2065\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2071\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1681\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1678\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1680\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1681\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1683\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1684\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/joblib/parallel.py:1799\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1789\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1794\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1797\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1798\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1799\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1800\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1802\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1803\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1810\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#residual regression on xgb errors\n",
    "# === MULTI-MODEL COMPARISON FOR BRAIN MRI AGE PREDICTION ===\n",
    "\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "# --- 1. Split (keep X_selected from RandomSubsetSelector) ---\n",
    "X_train, X_t, y_train, y_t = train_test_split(\n",
    "    X_selected, y_train_cleaned, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "X_train = X_train.astype(np.float32).values\n",
    "X_t = X_t.astype(np.float32).values\n",
    "y_train = y_train.astype(np.float32).values\n",
    "y_t = y_t.astype(np.float32).values\n",
    "\n",
    "# --- 2. CV Strategy ---\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "\n",
    "second_stage = {\n",
    "    \"TorchNN\": {\n",
    "            \"model\": NeuralNetRegressor(\n",
    "                module=MLP,\n",
    "                module__input_dim=X_selected.shape[1],\n",
    "                criterion=nn.MSELoss,\n",
    "                optimizer=optim.Adam,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu',  \n",
    "                verbose=1,\n",
    "                callbacks=[EarlyStopping(monitor='valid_loss', patience=50, threshold=0.0001)]\n",
    "\n",
    "            ),\n",
    "            \"params\": {\n",
    "                \"model__module__hidden_units\": [512, 1024],\n",
    "                \"model__module__num_layers\": [16, 32],\n",
    "                \"model__module__dropout\": [0, 0.1],\n",
    "                \"model__module__activation\" : [nn.Sigmoid,nn.ReLU],\n",
    "                \"model__optimizer__lr\": [0.002],\n",
    "                \"model__batch_size\": [128],\n",
    "                \"model__max_epochs\": [500]\n",
    "\n",
    "            },\n",
    "            \"needs_scaling\": True\n",
    "        },\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=35, n_jobs=-1, n_estimators=500, max_depth=5, learning_rate=0.02, subsample=0.8, colsample_bytree=0.7, gamma=1, reg_alpha=0.1,)\n",
    "\n",
    "# Proper scaling using fit_transform and transform methods\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_t = scaler_X.transform(X_t)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_t = scaler_y.transform(y_t.reshape(-1, 1)).flatten()\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_train_pred_xgb = xgb.predict(X_train)\n",
    "residuals = y_train - y_train_pred_xgb\n",
    "\n",
    "\n",
    "for model, config in second_stage.items():\n",
    "    # Build pipeline\n",
    "    steps = []\n",
    "    if config[\"needs_scaling\"]:\n",
    "        steps.append((\"scaler\", StandardScaler()))\n",
    "    steps.append((\"model\", config[\"model\"]))\n",
    "    pipe = Pipeline(steps)\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        pipe, config[\"params\"], cv=cv, scoring=scorer, n_jobs=-1, verbose=1\n",
    "    )\n",
    "    gs.fit(X_train, residuals)\n",
    "\n",
    "    y_pred_train_final = xgb.predict(X_train) + gs.predict(X_train).flatten()\n",
    "    y_pred_t_final = xgb.predict(X_t) + gs.predict(X_t).flatten()\n",
    "\n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train_final)\n",
    "    test_r2 = r2_score(y_t, y_pred_t_final)\n",
    "    test_mae = mean_absolute_error(y_t, y_pred_t_final)\n",
    "    results = []\n",
    "    results.append({\n",
    "        \"Model\": model,\n",
    "        \"Best R² (CV)\": gs.best_score_,\n",
    "        \"Train R²\": train_r2,\n",
    "        \"Test R²\": test_r2,\n",
    "        \"Test MAE\": test_mae,\n",
    "        \"Best Params\": gs.best_params_\n",
    "    })\n",
    "\n",
    "    print(f\"→ {model} | CV R²: {gs.best_score_:.4f} | Test R²: {test_r2:.4f} | Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# --- 5. Results Table ---\n",
    "results_df = pd.DataFrame(results).round(4)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results_df[[\"Model\", \"Best R² (CV)\", \"Train R²\", \"Test R²\", \"Test MAE\"]].to_string(index=False))\n",
    "print(results_df[[\"Best Params\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b33e9",
   "metadata": {},
   "source": [
    "# Train the model once again on the full dataset for the public test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafaaf37",
   "metadata": {},
   "source": [
    "# Save the test results for online evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14764918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__subsample': 0.6, 'model__reg_alpha': 0.6, 'model__n_estimators': 600, 'model__max_depth': 9, 'model__learning_rate': 0.02, 'model__gamma': 1, 'model__colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [10:38:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"model__colsample_bytree\", \"model__gamma\", \"model__learning_rate\", \"model__max_depth\", \"model__n_estimators\", \"model__reg_alpha\", \"model__subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to results/test_xgboost_20251104_103859.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kaggle competitions submit -c eth-aml-2025-project-1 -f submission.csv -m \"Message\"'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def save_test_predictions(model, model_name)-> Path:\n",
    "    model = final_training(model)\n",
    "    save_path = results_dir / f\"test_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "    y_test_pred = model.predict(X_test[X_selected.columns])\n",
    "    results_df = pd.DataFrame(y_test_pred, index=X_test.index, columns=[\"y\"])\n",
    "    assert (results_df.shape == (776, 1)), f\"Unexpected shape: {results_df.shape}\"\n",
    "    results_df.to_csv(save_path)\n",
    "    print(f\"Test predictions saved to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "\n",
    "file = save_test_predictions(XGBRegressor, \"xgboost\")\n",
    "\n",
    "\n",
    "\n",
    "'kaggle competitions submit -c eth-aml-2025-project-1 -f submission.csv -m \"Message\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5ed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.93k/9.93k [00:00<00:00, 25.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to ETH AML 2025 Project 1None None\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "#submit predictions\n",
    "def submit_predictions(file:Path, message:str) -> None:\n",
    "    cmd = f'kaggle competitions submit -c eth-aml-2025-project-1 -f {file} -m \"{message}\"'\n",
    "    res = subprocess.run(cmd, shell=True, check=True)\n",
    "    print(res.stdout, res.stderr)\n",
    "\n",
    "submit_msg = input(\"Enter submission message: \")\n",
    "submit_predictions(file, submit_msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LRF)",
   "language": "python",
   "name": "lrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
